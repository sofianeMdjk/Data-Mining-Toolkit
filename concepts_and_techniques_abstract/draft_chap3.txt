Now that we know more about our data, the next step is the preprocessing. It is important to measure the quality of the data that stands on three principles : accuracy, completeness and consistancy and in real life, it is common to find low quality data in large scale databases and warehouses. Pretending the data quality depends directly on the intended use of the data. Some other factors also affect data like timeliness, believability of the data and interpretability or the ease to understand data.
The preprocessing goes through seberal steps : 

-Data Cleaning : We start with the missing values.
Missing values : There's different methods to fill in the void :
		*Ignore the tuple (Usually done when the class label is missing), not the best method.
		*Fill in the missing value manually, time consuming and small scale operation.
		*Use a global constant to fill in the missing value.
		*Use the most probable value to fill in the missing value : may be found using regression or a decision tree using other related informations of the customer for example.
		*Use the attribute mean or median for all samples belonging to the same class as the given tuple.
It is to note that sometimes a missing value is not an error nor a misleading attribute. Now that we've gone through missing attributes, we face another problem which is noisy data.
Noisy data : To simply put it put it, a noise is an uncoherent value of an attribute depending on the context, and to get rid of that noise many techniques are possible : 
		*Binning: smoothes the value of the attribute by consulting the values around it, statistical methods like the mean, median and mode can be used for that.
		*Regression: involves finding the “best” line to fit two attributes so that one attribute can be used to predict the other.
		*Outlier analysis : usually using clustering, this is a technique that is described in an upcoming chapter of the textbook.
Data integration : used to reduces redundancies and inconstencies. Before starting the integration, we need to make sure that when it comes to merges from different databases, the structure of the data is coherent to ensure that any attribute functional dependencies and referential constraints in the source system match those in the target system. There's also the redundancy problem that can be detected with a coorelation analysis. One other common issue is the data value conflict. To put it simply, it is the values of the same attribute recorded in different places (common issue with data warehouses)that doesn't have the same value. e.g : in Algeria we record weight with kg and in the U.K with pounds.
Data Reduction : Generally we face huge amounts of data making the data analysis and this is where data reduction comes in to reduce the size of the dataset while maintaining its integrity. Reduction strategies are devided into : 
	*Dimetionnality reduction : it's the process of reducing the number of random variables or attributes under consideration, it includes wavelet transforms and principal component analysis methods.
	*Numerosity reduction : techniques replace the original data volume by alternative, smaller forms of data representation
	*Data compression  : Consists on having a compressed version of the data, If the original data can be reconstructed from the compressed data without any information loss, the data reduction is 		 called lossless. If, instead, we can reconstruct only an approximation of the original data, then the data reduction is called lossy.
 

	

